{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal RAG using a simple AI Agent with complex PDFÂ files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the environment variables we need to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Scripts\\python.exe\n",
      "2.10.6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pydantic\n",
    "print(pydantic.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "LLAMA_CLOUD_API_KEY = os.getenv(\"LLAMA_CLOUD_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llx-AccJAFOaMe92dbMLDRYsfaYACoCNRSA4t6qk83ZISOXyoTBf\n"
     ]
    }
   ],
   "source": [
    "print(LLAMA_CLOUD_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "Let's define the LLM model that we'll use as part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.2-vision:11b\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"llama3.2-vision:11b\"\n",
    "print(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing raw pdf using LlamaParse for getting Json Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "not_from_cache = False\n",
    "parser_txt = LlamaParse(verbose=True, invalidate_cache=not_from_cache, result_type=\"text\")\n",
    "parser_md = LlamaParse(verbose=True, invalidate_cache=not_from_cache, result_type=\"markdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing text...\n",
      "Started parsing the file under job_id c7a478ef-e4b2-4434-9b33-a6109c055fae\n",
      ".Parsing PDF file...\n",
      "Started parsing the file under job_id 39c87bd0-1019-4c2f-ad69-c6bf97dee52a\n"
     ]
    }
   ],
   "source": [
    "# # pdf_file = \"RatingScales_YBOCS_m.pdf\" # replace PDF file of interest\n",
    "\n",
    "# print(f\"Parsing text...\")\n",
    "# docs_text = parser_txt.load_data(pdf_file)\n",
    "# print(f\"Parsing PDF file...\")\n",
    "# md_json_objs = parser_md.get_json_result(pdf_file)\n",
    "# md_json_list = md_json_objs[0][\"pages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Current\n",
      "\n",
      "# Intrusive (non-violent) images\n",
      "\n",
      "# Intrusive nonsense sounds, words, or music\n",
      "\n",
      "# Bothered by certain sounds/noises *\n",
      "\n",
      "# Lucky/unlucky numbers\n",
      "\n",
      "# Colors with special significance\n",
      "\n",
      "# Superstitious fears\n",
      "\n",
      "# Other\n",
      "\n",
      "# SOMATIC OBSESSIONS\n",
      "\n",
      "# Concern with illness or disease *\n",
      "\n",
      "# Excessive concern with body part or aspect of appearance (e.g. dysmorphophobia) *\n",
      "\n",
      "# Other\n",
      "\n",
      "# CLEANING/WASHING COMPULSIONS\n",
      "\n",
      "# Excessive or ritualized handwashing\n",
      "\n",
      "# Excessive or ritualized showering, bathing, toothbrushing, grooming, or toilet routine.\n",
      "\n",
      "# Involves cleaning of household items or other inanimate objects\n",
      "\n",
      "# Other measures to prevent or remove contact with contaminants\n",
      "\n",
      "# Other\n",
      "\n",
      "# CHECKING COMPULSIONS\n",
      "\n",
      "# Checking locks, stove, appliances, etc.\n",
      "\n",
      "# Checking that did not/will not harm others\n",
      "\n",
      "# Checking that did not/will not harm self\n",
      "\n",
      "# Checking that nothing terrible did/will happen\n",
      "\n",
      "# Checking that did not make mistake\n",
      "\n",
      "# Checking tied to somatic obsessions\n",
      "\n",
      "# Other\n",
      "\n",
      "# REPEATING RITUALS\n",
      "\n",
      "# Re-reading or re-writing\n",
      "\n",
      "# Need to repeat routine activities (e.g. in/out door, up/down from chair)\n",
      "\n",
      "# Other\n",
      "\n",
      "# COUNTING COMPULSIONS\n",
      "\n",
      "# ORDERING/ARRANGING COMPULSIONS\n",
      "\n",
      "# HOARDING/COLLECTING COMPULSIONS\n",
      "\n",
      "# [distinguish from hobbies and concern with objects of monetary or sentimental value (e.g., carefully reads junkmail, piles up old newspapers, sorts through garbage, collects useless objects)]\n",
      "\n",
      "# MISCELLANEOUS COMPULSIONS\n",
      "\n",
      "# Mental rituals (other than checking/counting)\n",
      "\n",
      "# Excessive listmaking\n",
      "\n",
      "# Need to tell, ask, or confess\n",
      "\n",
      "# Need to touch, tap, or rub *\n",
      "\n",
      "# Rituals involving blinking or staring *\n",
      "\n",
      "# Measures (not checking) to prevent:\n",
      "\n",
      "- harm to self\n",
      "- harm to others\n",
      "- terrible consequences\n",
      "\n",
      "# Ritualized eating behaviors *\n",
      "\n",
      "# Superstitious behaviors\n",
      "\n",
      "# Trichotillomania *\n",
      "\n",
      "# Other self-damaging or self-mutilating behaviors *\n",
      "\n",
      "# Other\n",
      "\n",
      "v1.0 21 March 2014\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Output one page Json output for example \n",
    "# print(md_json_list[5][\"md\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Images for page 1: []\n",
      "> Images for page 2: []\n",
      "> Images for page 3: []\n",
      "> Images for page 4: []\n",
      "> Images for page 5: []\n",
      "> Images for page 6: []\n",
      "> Images for page 7: []\n",
      "> Images for page 8: []\n",
      "> Images for page 9: []\n",
      "> Images for page 10: []\n",
      "> Images for page 11: []\n",
      "> Images for page 12: []\n",
      "> Images for page 13: [{'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 36, 'y': 139.45996999999994, 'path': 'llm_images\\\\39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png', 'job_id': '39c87bd0-1019-4c2f-ad69-c6bf97dee52a', 'original_file_path': 'RatingScales_YBOCS_m.pdf', 'page_number': 13}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 37.44, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 38.88, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 40.32, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 41.76, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 43.2, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 44.64, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 46.08, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 47.52, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 48.96, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 50.4, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 51.84, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 53.28, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 54.72, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 56.16, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 57.6, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 59.04, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 60.48, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 61.92, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 63.36, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 64.8, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 66.24, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 67.704, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 69.144, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 70.584, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 72.024, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 73.464, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 74.904, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 76.344, 'y': 139.45996999999994}, {'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 77.784, 'y': 139.45996999999994}]\n",
      "> Images for page 14: []\n",
      "{'name': 'img_p12_2.png', 'height': 2, 'width': 4, 'x': 53.28, 'y': 139.45996999999994, 'path': 'llm_images\\\\39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png', 'job_id': '39c87bd0-1019-4c2f-ad69-c6bf97dee52a', 'original_file_path': 'RatingScales_YBOCS_m.pdf', 'page_number': 13}\n"
     ]
    }
   ],
   "source": [
    "# ### Extract images as dicts from parser\n",
    "# image_dicts = parser_md.get_images(md_json_objs, download_path=\"llm_images\")\n",
    "# # print one image dict as example\n",
    "# print(image_dicts[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit above cell to accommodate folder of PDFs\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "pdf_folder = \"data/documents\" # replace w folder path\n",
    "\n",
    "docs_text = []\n",
    "md_json_objs =[]\n",
    "image_dicts = []\n",
    "\n",
    "for file in tqdm(os.listdir(pdf_folder)):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        print(f\"Parsing text from {file}...\")\n",
    "        docs_text += parser_txt.load_data(os.path.join(pdf_folder, file))\n",
    "        print(f\"Parsing PDF file {file}...\")\n",
    "        md_json_objs += parser_md.get_json_result(os.path.join(pdf_folder, file))\n",
    "        image_dicts += parser_md.get_images(md_json_objs[-1], download_path=\"llm_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Multimodal Index\n",
    "In this section we build the multimodal index over the parsed deck.\n",
    "\n",
    "We do this by creating text nodes from the document that contain metadata referencing the original image path.\n",
    "\n",
    "In this example we're indexing the text node for retrieval. The text node has a reference to both the parsed text as well as the image screenshot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Text Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "'''\n",
    "Create a dictionary which maps page numbers to image paths with the following format:\n",
    "\n",
    "{\n",
    "    1: [Path(\"path/to/image1\"), Path(\"path/to/image2\")],    \n",
    "    2: [Path(\"path/to/image3\"), Path(\"path/to/image4\")],\n",
    "}\n",
    "'''\n",
    "def create_image_index(image_dicts):\n",
    "    image_index = {}\n",
    "\n",
    "    for image_dict in image_dicts:\n",
    "        page_number = image_dict[\"page_number\"]\n",
    "        image_path = Path(image_dict[\"path\"])\n",
    "        if page_number in image_index:\n",
    "            image_index[page_number].append(image_path)\n",
    "        else:\n",
    "            image_index[page_number] = [image_path]\n",
    "\n",
    "    return image_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "# attach image metadata to the text nodes\n",
    "def get_text_nodes(docs, json_dicts=None, image_dicts=None):\n",
    "    \"\"\"Split docs into nodes, by separator.\"\"\"\n",
    "    nodes = []\n",
    "    image_index = create_image_index(image_dicts) if image_dicts is not None else {}\n",
    "    print(\"Image index: \", image_index)\n",
    "    md_texts = [d[\"md\"] for d in json_dicts] if json_dicts is not None else None\n",
    "\n",
    "    doc_chunks = [c for d in docs for c in d.text.split(\"---\")]\n",
    "    page_num = 0\n",
    "    chunk_index = 0\n",
    "    while chunk_index < len(doc_chunks):\n",
    "        page_num += 1\n",
    "        chunk_metadata = {\"page_num\": page_num, \"image_paths\": []}\n",
    "        if image_index.get(page_num):\n",
    "            for image in image_index[page_num]:\n",
    "                chunk_metadata[\"image_paths\"].append(str(image))\n",
    "        if md_texts is not None:\n",
    "            chunk_metadata[\"parsed_text_markdown\"] = md_texts[chunk_index]\n",
    "        chunk_metadata[\"parsed_text\"] = doc_chunks[chunk_index]\n",
    "        node = TextNode(text=doc_chunks[chunk_index], metadata=chunk_metadata)\n",
    "        nodes.append(node)\n",
    "        chunk_index += 1\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image index:  {13: [WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png'), WindowsPath('llm_images/39c87bd0-1019-4c2f-ad69-c6bf97dee52a-img_p12_2.png')]}\n"
     ]
    }
   ],
   "source": [
    "# this will split into pages\n",
    "# text_nodes = get_text_nodes(docs_text, json_dicts=md_json_list, image_dicts=image_dicts)\n",
    "text_nodes = get_text_nodes(docs_text, json_dicts=md_json_objs, image_dicts=image_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_num: 7\n",
      "image_paths: []\n",
      "parsed_text_markdown: # TARGET SYMPTOM LIST\n",
      "\n",
      "# OBSESSIONS:\n",
      "\n",
      "1.\n",
      "2.\n",
      "3.\n",
      "\n",
      "# COMPULSIONS:\n",
      "\n",
      "1.\n",
      "2.\n",
      "3.\n",
      "\n",
      "# AVOIDANCE:\n",
      "\n",
      "1.\n",
      "2.\n",
      "3.\n",
      "\n",
      "v1.0 21 March 2014\n",
      "parsed_text: \n",
      "Na me      Dat e\n",
      "\n",
      "           TARGET SYMPTOM LIST\n",
      "\n",
      "OBSESSIONS:\n",
      "\n",
      "1.\n",
      "\n",
      "2.\n",
      "\n",
      "3.\n",
      "\n",
      "\n",
      "COMPULSIONS:\n",
      "\n",
      "1.\n",
      "\n",
      "2.\n",
      "\n",
      "3.\n",
      "\n",
      "\n",
      "AVOIDANCE:\n",
      "\n",
      "1.\n",
      "\n",
      "2.\n",
      "\n",
      "3.\n",
      "\n",
      "\n",
      "v1.0 21 March 2014\n",
      "\n",
      "\n",
      "Na me      Dat e\n",
      "\n",
      "           TARGET SYMPTOM LIST\n",
      "\n",
      "OBSESSIONS:\n",
      "\n",
      "1.\n",
      "\n",
      "2.\n",
      "\n",
      "3.\n",
      "\n",
      "\n",
      "COMPULSIONS:\n",
      "\n",
      "1.\n",
      "\n",
      "2.\n",
      "\n",
      "3.\n",
      "\n",
      "\n",
      "AVOIDANCE:\n",
      "\n",
      "1.\n",
      "\n",
      "2.\n",
      "\n",
      "3.\n",
      "\n",
      "\n",
      "v1.0 21 March 2014\n"
     ]
    }
   ],
   "source": [
    "print(text_nodes[6].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Index\n",
    "Once the text nodes are ready, we feed into our vector store index abstraction, which will index these nodes into a simple in-memory vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# set BAAI/bge-small-en-v1.5 as vector store embedding model \n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "vector_store_embedding = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "index = None\n",
    "if not os.path.exists(\"storage_nodes\"):\n",
    "    index = VectorStoreIndex(text_nodes, embed_model=vector_store_embedding)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage_nodes\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage_nodes\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\", embed_model=vector_store_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Multimodal Query Engine\n",
    "We now use LlamaIndex abstractions to build a custom query engine. In contrast to a standard RAG query engine that will retrieve the text node and only put that into the prompt (response synthesis module), this custom query engine will also load the image document, and put both the text and image document into the response synthesis module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# set LLama3.2-11b-visions as Ollama model and perform a sanity check if it is working\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm_model=Ollama(model=MODEL, request_timeout=500)\n",
    "response = llm_model.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import ImageNode, NodeWithScore, MetadataMode\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "QA_PROMPT_TMPL = \"\"\"\\\n",
    "Use the image(s) information first and foremost. ONLY use the text/markdown information provided in the context\n",
    "below if you can't understand the image(s).\n",
    "\n",
    "---------------------\n",
    "Context: {context_str}\n",
    "---------------------\n",
    "Given the context information and no prior knowledge, answer the query. Explain where you got the answer\n",
    "from, and if there's discrepancies, and your reasoning for the final answer.\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \"\"\"\n",
    "\n",
    "QA_PROMPT = PromptTemplate(QA_PROMPT_TMPL)\n",
    "\n",
    "class MultimodalQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"Custom multimodal Query Engine.\n",
    "\n",
    "    Takes in a retriever to retrieve a set of document nodes.\n",
    "    Also takes in a prompt template and multimodal model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    qa_prompt: PromptTemplate\n",
    "    retriever: BaseRetriever\n",
    "    multi_modal_llm: Ollama\n",
    "\n",
    "    def __init__(self, qa_prompt: Optional[PromptTemplate] = None, **kwargs) -> None:\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__(qa_prompt=qa_prompt or QA_PROMPT, **kwargs)\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        # retrieve text nodes\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "        # create ImageNode items from text nodes\n",
    "        image_nodes = [\n",
    "            NodeWithScore(node=ImageNode(image_path=image_path))\n",
    "            for n in nodes for image_path in n.metadata.get(\"image_paths\", [])\n",
    "        ]\n",
    "\n",
    "        # create context string from text nodes, dump into the prompt\n",
    "        context_str = \"\\n\\n\".join(\n",
    "            [r.get_content(metadata_mode=MetadataMode.LLM) for r in nodes]\n",
    "        )\n",
    "        fmt_prompt = self.qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "\n",
    "        image_docs = [image_node.node for image_node in image_nodes]\n",
    "        # synthesize an answer from formatted text and images\n",
    "        llm_response = self.multi_modal_llm.complete(\n",
    "            prompt=fmt_prompt,\n",
    "            image_documents=image_docs\n",
    "        )\n",
    "        return Response(\n",
    "            response=str(llm_response),\n",
    "            source_nodes=nodes,\n",
    "            metadata={\"text_nodes\": text_nodes, \"image_nodes\": image_nodes},\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = MultimodalQueryEngine(\n",
    "    retriever=index.as_retriever(similarity_top_k=5), multi_modal_llm=llm_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got the answer from the \"General Instructions\" section of the Y-BOCS scale, which states: \"This rating scale is designed to rate the severity and type of symptoms in patients with obsessive compulsive disorder (OCD).\"\n",
      "\n",
      "There are no discrepancies in this answer. The Y-BOCS scale is indeed designed to rate the severity and type of symptoms in patients with OCD.\n"
     ]
    }
   ],
   "source": [
    "# run a query\n",
    "response = query_engine.custom_query(\"What is the Y-BOCS scale designed for?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Multimodal Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set LLama3.1 as Ollama model for tool-calling since LLama3.2-vision currentlty does not support it\n",
    "# perform a sanity check if it is working\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm_model_tool_calling=Ollama(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can process text-based input, but I'm not capable of directly processing images. However, I can guide you on how to upload an image and provide information related to it if that's what you're looking for. If you'd like to discuss something specific about an image or ask a question about an image, feel free to provide more details or context.\n"
     ]
    }
   ],
   "source": [
    "response = llm_model_tool_calling.complete(\"Are you able to process image inputs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a large language model, I don't have direct control over external tools or their capabilities. However, I can try to provide information and guidance on how to achieve agentic tool-calling using various approaches.\n",
      "\n",
      "Llama3.2 is an open-source tool for creating and running agent-based models, which are simulations of complex systems that involve interacting agents with different goals and behaviors. While Llama3.2 does not have built-in support for agentic tool-calling in the classical sense (i.e., calling external tools to execute a specific action), there are some workarounds and alternatives:\n",
      "\n",
      "1. **Using external command-line tools**: You can use external command-line tools like `gnuplot`, `matplotlib`, or even shell scripts to create custom plots, visualize data, or perform other actions that require interaction with the user. Llama3.2 can be used as a backend engine for these tools.\n",
      "2. **Using API calls**: If you have access to an external tool or service that provides an API, you can use Llama3.2's internal API to call those services and execute specific actions. For example, if you're using the `gnuplot` tool via its API, you could use Llama3.2 to create a plot in a specific way.\n",
      "3. **Using custom scripts**: You can write custom scripts that interact with external tools or services using Llama3.2's internal APIs. This approach requires knowledge of both Llama3.2 and the external tool/service.\n",
      "\n",
      "Regarding the 3b version of Llama3.2, it's unclear whether this is a separate entity or an upgrade to an existing version. If you have any more information about what \"3b\" refers to, I may be able to provide more specific guidance.\n",
      "\n",
      "To answer your original question: if you're looking for agentic tool-calling capabilities in Llama3.2, it's unlikely that the 3b version would provide significant new functionality beyond those already available in Llama3.2. However, if you're experiencing issues with a specific external tool or service and want to use Llama3.2 as a backend engine, I can try to provide more information on how to set this up.\n"
     ]
    }
   ],
   "source": [
    "response = llm_model_tool_calling.complete(\"Are you able to do agentic tool-calling? If not, can the 3b version of llama3.2 do agentic tool-calling?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this currency converter seems unnecessary\n",
    "\n",
    "import requests\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "def currency_converter(from_currency_code: str = Field(\n",
    "        description=\"Country code of the currency to convert from (e.g., USD, INR, EUR)\"\n",
    "    ), to_currency_code: str = Field(\n",
    "        description=\"Country code of the currency to convert to (e.g., USD, INR, EUR)\"\n",
    "    ), amount: float = Field(\n",
    "        description=\"Currency amount to convert\"\n",
    "    )) -> float:\n",
    "\n",
    "    # free API for currency exchange rates\n",
    "    api_url = f\"https://api.vatcomply.com/rates?base={to_currency_code}\"\n",
    "    \n",
    "    response = requests.get(api_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    if \"error\" in data:\n",
    "        raise ValueError(data[\"error\"])\n",
    "    \n",
    "    rates = data[\"rates\"]\n",
    "    conversion_factor = rates[from_currency_code]\n",
    "    converted_amount = float(amount) / conversion_factor\n",
    "    return converted_amount\n",
    "\n",
    "# Tool for currency conversion\n",
    "currency_converter_tool = FunctionTool.from_defaults(\n",
    "    currency_converter,\n",
    "    name=\"currency_converter_tool\",\n",
    "    description=\"Converts currency from one country code to country code based on current exchange rate. \"\n",
    "    \"Takes the currency amount value, the country code of the currency to convert from, and the country code \"\n",
    "    \"of the currency to convert to as input.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool for querying the engine to retrieve contextual information around user query\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"query_engine_tool\",\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the data. Do NOT select if question asks for a summary of the data.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up the agent for calling the currency conversion and query engine tools\n",
    "agent = FunctionCallingAgentWorker.from_tools(\n",
    "    [currency_converter_tool, query_engine_tool], llm=llm_model_tool_calling, verbose=True\n",
    ").as_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What are compulsions?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"compulsions\"}\n",
      "=== Function Output ===\n",
      "I found the answer in the context information, specifically in the sections labeled \"7. INTERFERENCE DUE TO COMPULSIVE BEHAVIORS\", \"8. DISTRESS ASSOCIATED WITH COMPULSIVE BEHAVIOR\", \"9. RESISTANCE AGAINST COMPULSIONS\", and \"10. DEGREE OF CONTROL OVER COMPULSIVE BEHAVIOR\".\n",
      "\n",
      "These sections all relate to compulsive behaviors, and the answer is not explicitly stated. However, based on the context, I can infer that the answer is related to the questions and rating scales provided.\n",
      "\n",
      "The closest answer I can find is in section 6, which is labeled \"6. TIME SPENT PERFORMING COMPULSIVE BEHAVIORS\". However, this section does not provide a direct answer to the query.\n",
      "\n",
      "Therefore, I will not provide a specific answer to the query \"compulsions\". If you could provide more context or clarify the question, I would be happy to try and assist you further.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m query = (\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat are compulsions?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m response = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:52\u001b[39m, in \u001b[36mBaseQueryEngine.query\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     51\u001b[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m dispatcher.event(\n\u001b[32m     54\u001b[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001b[32m     55\u001b[39m )\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:42\u001b[39m, in \u001b[36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m callback_manager = cast(CallbackManager, callback_manager)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager.as_trace(trace_id):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\base\\agent\\types.py:49\u001b[39m, in \u001b[36mBaseAgent._query\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     agent_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m     54\u001b[39m         response=\u001b[38;5;28mstr\u001b[39m(agent_response), source_nodes=agent_response.source_nodes\n\u001b[32m     55\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:42\u001b[39m, in \u001b[36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m callback_manager = cast(CallbackManager, callback_manager)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager.as_trace(trace_id):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:695\u001b[39m, in \u001b[36mAgentRunner.chat\u001b[39m\u001b[34m(self, message, chat_history, tool_choice)\u001b[39m\n\u001b[32m    690\u001b[39m     tool_choice = \u001b[38;5;28mself\u001b[39m.default_tool_choice\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    692\u001b[39m     CBEventType.AGENT_STEP,\n\u001b[32m    693\u001b[39m     payload={EventPayload.MESSAGES: [message]},\n\u001b[32m    694\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     chat_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatResponseMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    701\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[32m    702\u001b[39m     e.on_end(payload={EventPayload.RESPONSE: chat_response})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:627\u001b[39m, in \u001b[36mAgentRunner._chat\u001b[39m\u001b[34m(self, message, chat_history, tool_choice, mode)\u001b[39m\n\u001b[32m    624\u001b[39m dispatcher.event(AgentChatWithStepStartEvent(user_msg=message))\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    626\u001b[39m     \u001b[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     cur_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cur_step_output.is_last:\n\u001b[32m    632\u001b[39m         result_output = cur_step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:423\u001b[39m, in \u001b[36mAgentRunner._run_step\u001b[39m\u001b[34m(self, task_id, step, input, mode, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# not clear when you would do that by theoretically possible\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == ChatResponseMode.WAIT:\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     cur_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_worker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == ChatResponseMode.STREAM:\n\u001b[32m    425\u001b[39m     cur_step_output = \u001b[38;5;28mself\u001b[39m.agent_worker.stream_step(step, task, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:42\u001b[39m, in \u001b[36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m callback_manager = cast(CallbackManager, callback_manager)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager.as_trace(trace_id):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\function_calling\\step.py:316\u001b[39m, in \u001b[36mFunctionCallingAgentWorker.run_step\u001b[39m\u001b[34m(self, step, task, **kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# get response and tool call (if exists)\u001b[39;00m\n\u001b[32m    309\u001b[39m response = \u001b[38;5;28mself\u001b[39m._llm.chat_with_tools(\n\u001b[32m    310\u001b[39m     tools=tools,\n\u001b[32m    311\u001b[39m     user_msg=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     allow_parallel_tool_calls=\u001b[38;5;28mself\u001b[39m.allow_parallel_tool_calls,\n\u001b[32m    315\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m tool_calls = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tool_calls_from_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_no_tool_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m tool_outputs: List[ToolOutput] = []\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._verbose \u001b[38;5;129;01mand\u001b[39;00m response.message.content:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Amzar\\Documents\\dcp\\llm-agentic-multimodal-rag\\.venv\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:312\u001b[39m, in \u001b[36mOllama.get_tool_calls_from_response\u001b[39m\u001b[34m(self, response, error_on_no_tool_call)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Predict and call the tool.\"\"\"\u001b[39;00m\n\u001b[32m    311\u001b[39m tool_calls = response.message.additional_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mtool_calls\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m)\u001b[49m < \u001b[32m1\u001b[39m:\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m error_on_no_tool_call:\n\u001b[32m    314\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    315\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected at least one tool call, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tool_calls)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tool calls.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    316\u001b[39m         )\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What are compulsions?\"\n",
    ")\n",
    "response = agent.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
